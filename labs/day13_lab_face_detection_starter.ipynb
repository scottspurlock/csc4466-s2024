{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq8F/KFDkgOCWAuVQwenq1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scottspurlock/csc4466-s2024/blob/main/labs/day13_lab_face_detection_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSC 4466 Computer Vision\n",
        "## Day 13 Lab: Face Detection\n",
        "The code below uses some fancy javascript to get an image from the webcam on your local machine and send it to the Jupyter notebook running in the cloud in Colab, where it can convert it to an image accessible in Python code.\n",
        "\n",
        "We will experiment with using Viola-Jones Haar Cascade models for face detection."
      ],
      "metadata": {
        "id": "Lxnotew7pOkh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pjrBkL-pKc3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Haar Cascade face detection model\n",
        "url = 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml'\n",
        "!curl -o haarcascade_frontalface_default.xml {url}\n"
      ],
      "metadata": {
        "id": "RfOQtKAtBCpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapted from: https://colab.research.google.com/drive/1QnC7lV7oVFk5OZCm75fqbLAfD9qBy9bw?usp=sharing#scrollTo=09b_0FAnUa9y\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "4NGXGGLdy0yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data)\n",
        "  return img"
      ],
      "metadata": {
        "id": "fT7dZO7D_-sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make sure webcam is working"
      ],
      "metadata": {
        "id": "ddgNNFFzEk01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a picture from the web cam and display it\n",
        "img = take_photo()\n",
        "rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(rgb)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EmC9X5s5AHex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Face detection"
      ],
      "metadata": {
        "id": "TIBCnU2gEpkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect faces\n",
        "\n",
        "# Take a picture from the webcam\n",
        "img = take_photo()\n",
        "\n",
        "# Make a copy of the image to draw on\n",
        "disp = img.copy()\n",
        "\n",
        "# Make a grayscale version for detection (the model was trained on gray images)\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Run detection\n",
        "# bounding_boxes is a list of 4-element tuples, e.g., [(x, y, w, h)]\n",
        "bounding_boxes = face_cascade.detectMultiScale(gray)\n",
        "\n",
        "# loop over each detection (bounding box)\n",
        "for (x, y, w, h) in bounding_boxes:\n",
        "    # draw the bounding box on the display image\n",
        "    disp = cv2.rectangle(disp, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "disp = cv2.cvtColor(disp, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(disp)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tf4lQPEWBnOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Experiment with failure cases\n",
        "Copy/paste the code to capture an image and run face detection 4 times."
      ],
      "metadata": {
        "id": "tgq3szuOEu0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Find and display 4 examples of failure cases for Haar Cascade face\n",
        "# detection. A failure case is when a face is not successfully detected. For\n",
        "# example, you might try changes in object pose (translation, rotation, scale,\n",
        "# expression), occlusion, and intra-class appearance (different people’s faces).\n",
        "\n"
      ],
      "metadata": {
        "id": "2UdD-xoxC6mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Answer this question based on your experiments.\n",
        "### To what extent is the Viola-Jones detector robust to changes in object pose (translation, rotation, scale, expression), occlusion, and intra-class appearance (different people’s faces)?\n",
        "\n",
        "Your answer here.\n"
      ],
      "metadata": {
        "id": "hqS_Y1f8DaqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Implement face swapping"
      ],
      "metadata": {
        "id": "l6UtjRZBFoMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To do: assuming that an image has two faces in it, write code to\n",
        "# swap the faces (first resizing each face to be the right size). You might ask\n",
        "# a neighbor to lean into the frame for this part, or you could hold your phone\n",
        "# with a face on it in the frame. Or, you might be surprised that a hand-drawn\n",
        "# face on a piece of paper could be detected.\n",
        "\n"
      ],
      "metadata": {
        "id": "BtunSeIK0lbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge: Blur Background\n",
        "Blur the background while keeping any detected face(s) unblurred. Try making a copy of the face region, blurring the entire image, then putting the copy back on top of the blurred image."
      ],
      "metadata": {
        "id": "wxtzvV_WjefO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge: Green Face\n",
        "1. Assume that the center pixel in a detected face is a good match for the face's overall skin color. Make a mask to threshold skin pixels in a range centered on this value. (It might be helpful to convert to some other color space.) Then tint the skin pixels green.\n",
        "2. Blur the background."
      ],
      "metadata": {
        "id": "aw1zLDCALFY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge: Experiment with other models\n",
        "Go to https://github.com/opencv/opencv/tree/master/data/haarcascades and download another model, e.g., haarcascade_eye.xml. (Click on \"raw\" to get a download url for the file.) Then update the link in the code snip below to download the model file. Run this new model on a new image from the webcam and display results.\n"
      ],
      "metadata": {
        "id": "IHvgjy-yE9gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: update URL to point to a new model\n",
        "# Download the Haar Cascade detection model\n",
        "url = 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml'\n",
        "!curl -o haarcascade_frontalface_default.xml {url}\n"
      ],
      "metadata": {
        "id": "MVwrYid2HAIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: experiment with a different Haar Cascade model.\n"
      ],
      "metadata": {
        "id": "VWDhjADVFjKN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}